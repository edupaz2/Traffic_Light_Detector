{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "\n",
    "## Project: Build a Traffic Light Detector for CarND-Capstone project\n",
    "\n",
    "In this notebook, we are building a Traffic Light Detector for the Capstone project for the Udacity Self Driving Cars nanodegree. \n",
    "\n",
    "In addition to implementing code, there is a [write up file](https://github.com/edupaz2/Traffic-Light-Classifier-Project/blob/master/writeup.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def img2png(jpgs_list, remove=True):\n",
    "    for j in jpgs_list:\n",
    "        # Convert to PNG\n",
    "        img = cv2.imread(j)\n",
    "        print(j[:-3] + 'png')\n",
    "        cv2.imwrite(j[:-3] + 'png', img)\n",
    "        # Delete the file\n",
    "        if remove:\n",
    "            os.remove(j)\n",
    "\n",
    "def img2jpg(jpgs_list, remove=True):\n",
    "    for j in jpgs_list:\n",
    "        # Convert to PNG\n",
    "        img = cv2.imread(j)\n",
    "        print(j[:-3] + 'jpg')\n",
    "        cv2.imwrite(j[:-3] + 'jpg', img)\n",
    "        # Delete the file\n",
    "        if remove:\n",
    "            os.remove(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../0.Capstone/shared/img_292_5.jpg\n"
     ]
    }
   ],
   "source": [
    "img2jpg(['../0.Capstone/shared/img_292_5.png'], remove=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform JPG to PNG\n",
    "transformJPG2PNG = False\n",
    "if transformJPG2PNG:\n",
    "    l = glob.glob('./sim-data/**/*.jpg', recursive=True)\n",
    "    img2png(l)\n",
    "\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename files Consecutively\n",
    "\n",
    "def renameConsecutively(path):\n",
    "    counter = 1\n",
    "    filelist = os.listdir(path)\n",
    "    zerofill = len(str(abs(len(filelist))))\n",
    "    for f in filelist:\n",
    "        oldname = '{0}/{1}'.format(path, f)\n",
    "        newname = '{0}/{1}.png'.format(path, str(counter).zfill(zerofill))\n",
    "        os.rename(oldname, newname)\n",
    "        counter += 1\n",
    "\n",
    "rename = False\n",
    "if rename:\n",
    "    renameConsecutively('sim-data/green')\n",
    "    renameConsecutively('sim-data/none')\n",
    "    renameConsecutively('sim-data/red')\n",
    "    renameConsecutively('sim-data/yellow')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the pickle data\n",
    "At this point we must have:\n",
    "- Directory 'sim-data' containing the pictures in PNG format.\n",
    "- Inside 'sim-data', 4 folders: Green, None, Red, Yellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to pickle file...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "size = 128, 128\n",
    "\n",
    "images  = glob.glob('./sim-data/**/*.png', recursive=True)\n",
    "\n",
    "for image_file in images:\n",
    "    image = Image.open(image_file)\n",
    "    image.thumbnail(size, Image.ANTIALIAS)\n",
    "    #image.load()\n",
    "    # Load image data as 1 dimensional array\n",
    "    # We're using float32 to save on memory space\n",
    "    feature = np.array(image, dtype=np.float32)#.flatten()\n",
    "    labelStr = os.path.split(os.path.split(image_file)[0])[1]\n",
    "    if labelStr == 'green':\n",
    "        labels.append(1)\n",
    "    elif labelStr == 'red':\n",
    "        labels.append(2)\n",
    "    elif labelStr == 'yellow':\n",
    "        labels.append(3)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "\n",
    "    features.append(feature)\n",
    "\n",
    "# Save the data for easy access\n",
    "pickle_file = 'sim-data/sim-data.pickle'\n",
    "print('Saving data to pickle file...')\n",
    "try:\n",
    "    with open(pickle_file, 'wb+') as pfile:\n",
    "        pickle.dump(\n",
    "            {\n",
    "                'features': features,\n",
    "                'labels': labels,\n",
    "            },\n",
    "            pfile, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise\n",
    "\n",
    "# Free up memory\n",
    "del pickle_file\n",
    "del features\n",
    "del labels\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 1121, Labels: 1121, Image_Shape: (96, 128, 3), Classes: 4\n"
     ]
    }
   ],
   "source": [
    "# Reload the data\n",
    "pickle_file = 'sim-data/sim-data.pickle'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    pickle_data = pickle.load(f)\n",
    "    features = pickle_data['features']\n",
    "    labels = pickle_data['labels']\n",
    "    del pickle_data  # Free up memory\n",
    "\n",
    "# Number of features\n",
    "n_features = len(features)\n",
    "\n",
    "# Number of labels\n",
    "n_labels = len(labels)\n",
    "\n",
    "# What's the shape of an traffic sign image?\n",
    "image_shape = features[0].shape\n",
    "\n",
    "# How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(labels))\n",
    "\n",
    "print('Features: {0}, Labels: {1}, Image_Shape: {2}, Classes: {3}'.format(n_features, n_labels, image_shape, n_classes))\n",
    "\n",
    "del pickle_file\n",
    "del features\n",
    "del labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Pickle...\n",
      "Features: 1121, Labels: 1121, Image_Shape: (96, 128, 3), Classes: 4\n",
      "Done\n",
      "2. Augmenting data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "3. Validating data...\n",
      "Feature: 1121\n",
      "Feature shape: (96, 128, 3)\n",
      "-------------\n",
      "Augmented features: (3400, 96, 128, 3)\n",
      "Augmented features shape: (96, 128, 3)\n",
      "Done\n",
      "4. Saving data to pickle file...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "### Preprocess the data here. It is required to normalize the data. Other preprocessing steps could include \n",
    "### converting to grayscale, etc.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class ImagePreprocessor():\n",
    "    \n",
    "    toGrayMask = np.array((0.299, 0.587, 0.114))\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def preprocess(self, image_data):\n",
    "        # GrayScale and Normalize the input\n",
    "        return self.normalize_grayscale(self.image_to_gray(image_data))\n",
    "\n",
    "    def image_to_gray(self, image_data):\n",
    "        gray = np.dot(image_data[...,:3], self.toGrayMask)\n",
    "        gray = gray.squeeze()\n",
    "        gray = gray.reshape(32, 32, 1)\n",
    "        return gray\n",
    "\n",
    "    def normalize_grayscale(self, image_data):\n",
    "        return (image_data-128)/128\n",
    "\n",
    "    # Functions extracted from cv2 doc\n",
    "    # http://opencv-python-tutroals.readthedocs.io/en/latest/ \\\n",
    "    # py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html\n",
    "    def translate(self, image_data):\n",
    "        rows,cols = image_data.shape[0], image_data.shape[1]\n",
    "        x_trans, y_trans = np.random.uniform(-2, 2), np.random.uniform(-2, 2)\n",
    "        M = np.float32([[1, 0, x_trans],[0, 1, y_trans]])\n",
    "        return cv2.warpAffine(image_data,M,(cols,rows))\n",
    "\n",
    "    def rotate(self, image_data):\n",
    "        rows,cols = image_data.shape[0], image_data.shape[1]\n",
    "        rotate_factor = np.random.uniform(-15, 15)\n",
    "        M = cv2.getRotationMatrix2D((cols/2,rows/2), rotate_factor, 1)\n",
    "        return cv2.warpAffine(image_data,M,(cols,rows))\n",
    "\n",
    "    def perspective(self, image_data):\n",
    "        rows,cols = image_data.shape[0], image_data.shape[1]\n",
    "        fctr = np.random.uniform(0.05*rows, 0.2*rows)\n",
    "        pts1 = np.float32([[fctr,fctr],[rows-fctr,fctr],[fctr,cols-fctr],[rows-fctr,cols-fctr]])\n",
    "        pts2 = np.float32([[0,0],[rows,0],[0,cols],[rows,cols]])\n",
    "        M = cv2.getPerspectiveTransform(pts1,pts2)\n",
    "        return cv2.warpPerspective(image_data, M, (cols, rows))\n",
    "    \n",
    "    def affine(self, image_data):\n",
    "        rows,cols = image_data.shape[0], image_data.shape[1]\n",
    "        rnd1x, rnd1y = np.random.uniform(0, rows*0.25), np.random.uniform(0, cols*0.25)\n",
    "        rnd2x, rnd2y = np.random.uniform(rows*0.75, rows), np.random.uniform(0, cols*0.35)\n",
    "        rnd3x, rnd3y = np.random.uniform(0, rows*0.35), np.random.uniform(cols*0.75, cols)\n",
    "        pts1 = np.float32([[0,0],[rows-1,0],[0,cols-1]])\n",
    "        pts2 = np.float32([[rnd1x, rnd1y],[rnd2x, rnd2y],[rnd3x, rnd3y]])\n",
    "        M = cv2.getAffineTransform(pts1,pts2)\n",
    "        return cv2.warpAffine(image_data, M, (cols,rows))\n",
    "    \n",
    "    def research(self, image_data):\n",
    "        # Define a kernel size and apply Gaussian smoothing\n",
    "        #kernel_size = 5\n",
    "        #return cv2.GaussianBlur(image_data.squeeze(),(kernel_size, kernel_size),0)\n",
    "        \n",
    "        #image_data = cv2.cvtColor(image_data, cv2.COLOR_BGR2GRAY)\n",
    "        #return cv2.adaptiveThreshold(image_data,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "        \n",
    "        return cv2.bitwise_not(image_data)\n",
    "\n",
    "    def random_effect(self, image_data):\n",
    "        # Apply just one effect at a time\n",
    "        effects = [self.translate, self.rotate, self.perspective, self.affine, self.research]\n",
    "        effects_p = [0.2, 0.2, 0.25, 0.25, 0.1]\n",
    "        effects_count = random.randint(2, len(effects)) # at least 2 effects\n",
    "        effects_toapply = np.random.choice(len(effects), effects_count, replace=False, p=effects_p)\n",
    "        for idx in effects_toapply:\n",
    "            image_data = effects[idx](image_data)\n",
    "\n",
    "        return image_data\n",
    "\n",
    "print('1. Loading Pickle...')\n",
    "# Reload the data\n",
    "pickle_file = 'sim-data/sim-data.pickle'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    pickle_data = pickle.load(f)\n",
    "    features = pickle_data['features']\n",
    "    labels = pickle_data['labels']\n",
    "    del pickle_data  # Free up memory\n",
    "\n",
    "# Number of features\n",
    "n_features = len(features)\n",
    "\n",
    "# Number of labels\n",
    "n_labels = len(labels)\n",
    "\n",
    "# What's the shape of an traffic sign image?\n",
    "image_shape = features[0].shape\n",
    "\n",
    "# How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(labels))\n",
    "\n",
    "print('Features: {0}, Labels: {1}, Image_Shape: {2}, Classes: {3}'.format(n_features, n_labels, image_shape, n_classes))\n",
    "\n",
    "print('Done')\n",
    "print('2. Augmenting data...')\n",
    "### Let´s add some random data to make our training data bigger\n",
    "# Find out the class with more items, and increase all classes 35%\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "train_data_idxs = defaultdict(list)\n",
    "for i in range(len(features)):\n",
    "    train_data_idxs[labels[i]].append(i)\n",
    "\n",
    "features_aug_l = []\n",
    "labels_aug_l = []\n",
    "preprocessor = ImagePreprocessor()\n",
    "\n",
    "hist, bin_edges = np.histogram(labels, bins=n_classes)\n",
    "increase_to = int(np.amax(hist)*1.5)\n",
    "for labelidx in range(n_classes):\n",
    "    # Iterate in each class\n",
    "    #to_generate = random.randint(hist[labelidx], increase_to)\n",
    "    to_generate = increase_to - hist[labelidx]\n",
    "    for j in range(to_generate):\n",
    "        # Pick a random image of the same class\n",
    "        rnd_idx = np.random.choice(train_data_idxs[labelidx])\n",
    "        rnd_img = features[rnd_idx]\n",
    "        # Do a random effect\n",
    "        image = preprocessor.random_effect(rnd_img)\n",
    "        # Add it to training data\n",
    "        features_aug_l.append(image)\n",
    "        labels_aug_l.append(labelidx)\n",
    "        \n",
    "# Everyday Im shuffling\n",
    "features_aug_l, labels_aug_l = shuffle(features_aug_l, labels_aug_l)\n",
    "print('Done')\n",
    "\n",
    "print('3. Validating data...')\n",
    "print('Feature: {0}'.format(len(features)))\n",
    "print('Feature shape: {0}'.format(features[0].shape))\n",
    "print('-------------')\n",
    "\n",
    "features_aug = np.append(features, features_aug_l, axis=0)\n",
    "del features\n",
    "del features_aug_l\n",
    "\n",
    "labels_aug = np.append(labels, labels_aug_l)\n",
    "del labels\n",
    "del labels_aug_l\n",
    "\n",
    "print('Augmented features: {0}'.format(features_aug.shape))\n",
    "print('Augmented features shape: {0}'.format(features_aug[0].shape))\n",
    "print('Done')\n",
    "\n",
    "### Then save it so we can start from there later\n",
    "# Save the data for easy access\n",
    "\n",
    "pickle_file = 'sim-data/sim-data-augmented.pickle'\n",
    "print('4. Saving data to pickle file...')\n",
    "try:\n",
    "    with open(pickle_file, 'wb+') as pfile:\n",
    "        pickle.dump(\n",
    "            {\n",
    "                'features': features_aug,\n",
    "                'labels': labels_aug,\n",
    "            },\n",
    "            pfile, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise\n",
    "\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
